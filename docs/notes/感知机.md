有监督二分类线性模型。
# 算法原理

+ 线性判别
	+ $$判别函数: \ t=f(\sum_{i=1}^nw_ix_i+b)=f(W^TX)$$
	+ $$分两类：\ f(n)=\begin{cases} +1& \text n\geq0\\-1& \text n\lt0 \end{cases}$$
	+ $$w_1x_1+w_2x_2+w_3x_3+...+b=0\  即\ \sum_{i=1}^nw_ix_i+b=0\ 表示分界面 $$
	+ $$其中\ \ W^T=\begin{bmatrix} b & w_1 &w_2& ... &w_n \end{bmatrix} \ \ \ \ \ \ \ X=\begin{bmatrix} 1 \\ x_1 \\x_2\\ ... \\x_n \end{bmatrix}$$
+ 损失函数
	+ $$对于超平面wx+b=0,点x_0 到超平面S_0的距离为\ \frac{|wx_0+b|}{\Vert w\Vert}$$
	+ 对于误分类的样本点$(x_i,y_i)$，有$\ -y_i(wx_i+b) = |wx_i+b| \gt 0$
	+ 因此误分类点到超平面距离可表示为：$$\frac{-y_i(wx_i+b)}{\Vert w \Vert}$$
	+ 假设M为所有误分类点集合，所有误分类点到超平面距离总和为：$$\sum_{x\in M}\frac{-y_i(wx_i+b)}{\Vert w \Vert}$$
	+ 损失函数为：$$L(w,b)=\sum_{x\in M} {-y_i(wx_i+b)}$$
	+ 对损失函数求偏导：$$\frac{\partial L(w,b)}{\partial w}=- \sum_{x_i\in M}y_ix_i \ \ \ \ \ \ \  \frac{\partial L(w,b)}{\partial b}=-\sum_{x_i \in M}y_i$$
	+ 设 α 为学习率，则w、b的梯度下降迭代公式为：$$w=w+\alpha \sum_{x_i \in M}y_ix_i \ \ \ \ \ \ \ \ \  b=b+\alpha \sum_{x_i \in M}y_i$$


# 算法收敛性
当训练数据集线性可分时，感知机学习算法是收敛的。感知机算法在训练数据集上的误分类次数 k 满足不等式：$$ k \le \frac{R^2}{\gamma^2}$$R为是样本集中最大特征向量的长度。**γ** 是分隔超平面到样本集的最小距离。

# 代码

```python
import numpy as np  
import matplotlib.pyplot as plt  
  
# 定义样本数据  
omega_1 = np.array([  
    [0, 0, 0],  
    [1, 0, 0],  
    [1, 0, 1],  
    [1, 1, 0]  
])  
  
omega_2 = np.array([  
    [0, 0, 1],  
    [0, 1, 1],  
    [0, 1, 0],  
    [1, 1, 1]  
])  
  
# 加上偏置项，统一表示为 (x1, x2, x3, 1)，并转换为浮点类型  
omega_1 = np.hstack((omega_1[:, :3].astype(float), np.ones((omega_1.shape[0], 1), dtype=float)))  
omega_2 = np.hstack((omega_2[:, :3].astype(float), np.ones((omega_2.shape[0], 1), dtype=float)))  
  
# 初始化权重向量 ww = np.array([-1, -2, -2, 0], dtype=float)  
learning_rate = 1  # 学习率  
max_epochs = 100  # 最大迭代次数  
  
# 感知器算法  
for epoch in range(max_epochs):  
    error_count = 0  
    for i in range(omega_1.shape[0] + omega_2.shape[0]):  
        x = omega_1[i] if i < omega_1.shape[0] else omega_2[i - omega_1.shape[0]]  
        y = 1 if i < omega_1.shape[0] else -1  
        y_hat = np.sign(np.dot(w, x))  
        if y_hat != y:  
            w += learning_rate * y * x  
            error_count += 1  
    if error_count == 0:  
        print("yes~")  
        break  
  
# 可视化结果 - 三维图  
fig = plt.figure()  
ax = fig.add_subplot(111, projection='3d')  
  
# 绘制 omega_1 和 omega_2 样本点  
ax.scatter(omega_1[:, 0], omega_1[:, 1], omega_1[:, 2], color='blue', label=r'$\omega_1$', s=50)  
ax.scatter(omega_2[:, 0], omega_2[:, 1], omega_2[:, 2], color='red', label=r'$\omega_2$', s=50)  
  
# 定义分类平面  
xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 10), np.linspace(-0.5, 1.5, 10))  
if w[2] != 0:  # 防止除以0  
    zz = -(w[0] * xx + w[1] * yy + w[3]) / w[2]  # 计算 z 值  
  
    # 绘制分类平面  
    ax.plot_surface(xx, yy, zz, color='gray', alpha=0.5, rstride=100, cstride=100)  
  
# 图形设置  
ax.set_xlabel(r'$x_1$')  
ax.set_ylabel(r'$x_2$')  
ax.set_zlabel(r'$x_3$')  
ax.legend()  
ax.set_title("3D Perceptron Classification Result")  
  
plt.show()  
  
print(w)
```
运行结果：
![](./imgs/Pasted%20image%2020241109223933.png)